# svm
## [code](SVM.py)

# 交叉验证
> 解决缺点一:浪费数据
解决缺点二:容易过拟合,且矫正方式不方便
这时,我们需要使用另外一种分割方式-交叉验证
## [code](cross_val_score.py)

# KFold
> 我们不需要从k模型中选择一个模型，但我们可以通过利用bagging（三种Ensemble方法之一）将k模块合二为一。<br/>
> question:<br/>
>我用k折（k=10）之后产生了10个训练完成的模型，然后我想现在选一个保存以后用，那保存哪个呢？<br/>
>这十个模型是大同小异还是存在最好的模型呢？<br/>
>answer:<br/>
>使用K-fold交叉验证的目的是从不同模型（例如线性回归、随机森林、GBDT等等）或者<br/>
> 同一模型的不同超参数组合中选择最优的那一个（每个模型或者超参数组合训练K次，在K个验证集上的误差做平均，<br/>
>平均误差最小的那一个）<br/>
> 之后将最优的这个使用整个训练集进行训练，即是你最终得到的模型<br/>
## [code](KFold.py)

# KNN
## [code](knn.py)