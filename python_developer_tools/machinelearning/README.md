# svm
## [code](SVM.py)

# 交叉验证
> 解决缺点一:浪费数据
解决缺点二:容易过拟合,且矫正方式不方便
这时,我们需要使用另外一种分割方式-交叉验证
## [code](cross_val_score.py)

# KFold
> 我们不需要从k模型中选择一个模型，但我们可以通过利用bagging（三种Ensemble方法之一）将k模块合二为一。<br/>
> question:<br/>
>我用k折（k=10）之后产生了10个训练完成的模型，然后我想现在选一个保存以后用，那保存哪个呢？<br/>
>这十个模型是大同小异还是存在最好的模型呢？<br/>
>answer:<br/>
>使用K-fold交叉验证的目的是从不同模型（例如线性回归、随机森林、GBDT等等）或者<br/>
> 同一模型的不同超参数组合中选择最优的那一个（每个模型或者超参数组合训练K次，在K个验证集上的误差做平均，<br/>
>平均误差最小的那一个）<br/>
> 之后将最优的这个使用整个训练集进行训练，即是你最终得到的模型<br/>
## [code](KFold.py)
## 为什么说交叉验证能 防止过拟合呢？
>交叉验证 把 原始数据分成可 K 块 ，用每次用其中的 K-1块训练， 用 余下的 一块进行预测，<br/>
这样做，让算法在训练的时候没有接触过 余下来的那一块数据子集中的内容，这样，<br/>
>在对 余下这一块进行预测时，
算法只能靠他学会的本领来进行预测---------> 就是用学到的参数来预测。<br/>
但是，算法还是有超参数的：我们每次都需要主观选择超参数给算法才行，<br/>
>但是不同的超参数会导致模型有不同的效果，假设我们有四个超参数【1，10，100，1000】，<br/>
>我们运行10折交叉验证（既 训练了40个模型），  每次我们都用10个训练出来的模型的平均值 来表示 当前 超参数的效果。   <br/> 
所以 我们比较了  4 个超参数的效果后，我们能找到一个更加 好的超参数，我们再用全量的数据 在该超参数上训练模型。<br/>
>因此，这样训练出来的模型是相对更优的，在一定程度上能避免 过拟合问题。<br/>

# KNN
## [code](knn.py)